#!/usr/bin/env python
# -*- coding: UTF-8 -*-

 
from misc import bcolors

#from pygoogle import pygoogle

import mechanize
from bs4 import BeautifulSoup 



class pentest:
    def __init__(self):
        #print ""
        vnum = 1
        
    def pentestWebSite(self, webSiteUrl, dorksFound):
        # Browser instantiation
        browsing = mechanize.Browser()
        browsing.set_handle_robots(False)   # ignore robots
        browsing.set_handle_refresh(False)  # can sometimes hang without this
        browsing.addheaders = [('User-agent', 'Mozilla/5.0 (Windows NT 6.0; rv:30.0) Gecko/20100101 Firefox/30.0')]
        
        # List of vulnerable url & dork
        urlList = []
        
    
        print "["+bcolors.OKBLUE+"*"+bcolors.ENDC+"] Pentesting Url : %s" % webSiteUrl
        for dork in dorksFound:
            sep_car = "%20"
            link = str(dork[2])
            link = link.replace("q=", "q=site:"+webSiteUrl+sep_car)            
            #link = "http://www.google.com/search?q=site:"+webSiteUrl+sep_car #for debug
            print "[*] Testing with dork : %s" % link
            
            # pygoogle Method but terms of use restriction
            #google_search = pygoogle(title)
            #google_search.pages = 10
            #if google_search.get_result_count() == 0:
            #    print "[-] No result"
            #else:
            #    print "[-] Found %s result(s)" % google_search.get_result_count()
            #    for url in google_search.get_urls():
            #        print url
            #time.sleep(randint(2,10)) 
            
            #Mechanize Method
            positiveResult = 0
            error_Http = 0
            try:
                response_browsing = browsing.open(link)
            except Exception as e:
                #print e.code
                if e.code == 503:
                    print bcolors.WARNING+"[-] Service unavailable. Could be a Captcha. Pass to next dork!"+bcolors.ENDC
                    error_Http = 1
                    
            if error_Http == 0:        
                response_browsing.encoding = "UTF-8"
                if response_browsing.code == 200:
                    soup = BeautifulSoup(browsing.response().read())
                    #print browsing.response().read()
                    #print soup.findAll('h3',{'class':'r'})
                    links = soup.findAll('h3',{'class':'r'})
                    #linkh3 = soup.findAll("h3").find("a").get("data-href")
                    for link in links:
                        
                        urlVuln = link.find("a").get("href")
                                           
                        if urlVuln[7:len(webSiteUrl)+7] == webSiteUrl:
                            print "["+bcolors.OKGREEN+"+"+bcolors.ENDC+"] Vulnerable URL found for this website." 
                            print "["+bcolors.OKGREEN+"*"+bcolors.ENDC+"] Url : %s " % urlVuln
                            urlList.append(urlVuln)
                            positiveResult = 1
                    
                    if positiveResult == 0:
                        print "["+bcolors.FAIL+"-"+bcolors.ENDC+"] No result found" 
                        #if links_result[0:len(webSiteUrl)] == webSiteUrl:
            
                    
        if len(urlList) > 0:
            print "["+bcolors.OKBLUE+"*"+bcolors.ENDC+"] The vulnerable urls are : \r\n"
            for urlVulnResult in urlList:            
                print urlVulnResult 
            
        
        
